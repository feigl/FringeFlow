{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca9ac1c",
   "metadata": {},
   "source": [
    "#### Select and create InSAR interferograms with ASF Search \n",
    "version 2 look \n",
    "cut pairs to area of interest, not overlapIntersect\n",
    "do not omit seasons\n",
    "short time spans only\n",
    "\n",
    "2025/05/21 make small test case to debug pyaps\n",
    "2025/06/06 use UTM zones consistently\n",
    "2025/06/09 try to generalize\n",
    "2025/08/31 clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d711541",
   "metadata": {},
   "source": [
    " #### 0. Initial Setup\n",
    "\n",
    "To run this notebook, you'll need a conda environment with the required dependencies. You can set up a new environment (recommended) and run the jupyter server like:\n",
    "```shell\n",
    "mamba create --name hyp3-mintpy \"python>=3.10\" \"asf_search>=7.0.0\" hyp3_sdk \"mintpy>=1.5.2\" pandas jupyter ipympl jupytext gdal proj  --channel conda-forge --yes\n",
    "```\n",
    "\n",
    "mamba run -n hyp3kf jupytext --to py hyp3_insar_stack_for_ts_v1.ipynb\n",
    "\n",
    "To run in VS Code, then use command palette to do the following:\n",
    "    Clear Workspace Interpreter Setting\n",
    "    Select interpreter\n",
    "        Type interpreter\n",
    "            \n",
    "   ~/miniforge3/envs/hyp3-mintpy/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dateutil.parser import parse as parse_date\n",
    "import datetime\n",
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "import hyp3_sdk as sdk\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "#from osgeo import gdal # Import \"osgeo\" could not be resolved\n",
    "from osgeo import gdal, osr\n",
    "#import gdal \n",
    "import numpy as np\n",
    "import shapely \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from pyproj import Proj, transform, CRS, Transformer\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "from dateutil.parser import parse as parse_date\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_dims(sitecode5):\n",
    "    \"\"\"\n",
    "    Return bounding box and UTM limits from site_dims.txt for given 5-character site code.\n",
    "    Equivalent to MATLAB function by Kurt Feigl (2021/10/18).\n",
    "    \"\"\"\n",
    "    home = os.environ.get('HOME')\n",
    "    fname = os.path.join(home, 'siteinfo', 'site_dims.txt')\n",
    "\n",
    "    LIMITS = {}\n",
    "    try:\n",
    "        with open(fname, 'rt') as fid:\n",
    "            lines = fid.readlines()\n",
    "\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            tline = lines[i].strip()\n",
    "            if len(tline) == 6 and ':' in tline and tline[:5].lower() == sitecode5.lower():\n",
    "                # Found matching site\n",
    "                if i + 3 >= len(lines):\n",
    "                    raise ValueError(\"Insufficient lines after match in file.\")\n",
    "\n",
    "                # Line 1: lat/lon\n",
    "                latlon_line = lines[i+1].strip().replace('-R', '').replace('/', ',')\n",
    "                lonMin, lonMax, latMin, latMax = map(float, latlon_line.split(','))\n",
    "                LIMITS['lonMin'] = lonMin\n",
    "                LIMITS['lonMax'] = lonMax\n",
    "                LIMITS['latMin'] = latMin\n",
    "                LIMITS['latMax'] = latMax\n",
    "\n",
    "                # Line 2: UTM\n",
    "                utm_line = lines[i+2].strip().replace('-R', '').replace('/', ',')\n",
    "                Emin, Emax, Nmin, Nmax = map(float, utm_line.split(','))\n",
    "                LIMITS['Emin'] = Emin\n",
    "                LIMITS['Emax'] = Emax\n",
    "                LIMITS['Nmin'] = Nmin\n",
    "                LIMITS['Nmax'] = Nmax\n",
    "\n",
    "                # Line 3: UTM zone\n",
    "                utm_zone_line = lines[i+3].strip()\n",
    "                LIMITS['UTMzone'] = int(utm_zone_line)\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if not LIMITS:\n",
    "            raise ValueError(f\"Site code {sitecode5} not found in {fname}\")\n",
    "\n",
    "        return LIMITS\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not open file named {fname}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_year(date): # function to calculate the day of the year\n",
    "    #print(f\"date is {date}\")\n",
    "    #print(f'type of date is {type(date)}')\n",
    "    if type(date) == type(datetime.datetime(2024,8,29)):\n",
    "        doy = date - datetime.datetime(date.year, 1, 1,tzinfo=date.tzinfo) + datetime.timedelta(days=1)\n",
    "    elif type(date) == type('hello'):\n",
    "        date1=parse_date(date)\n",
    "        doy = date1 - datetime.datetime(date1.year, 1, 1, tzinfo=date1.tzinfo) + datetime.timedelta(days=1)\n",
    "    elif type(date) == 'pandas.core.series.Series':\n",
    "        nDates=len(date)\n",
    "        print(f\"nDates is {nDates}\")\n",
    "    else:\n",
    "        print(f\"Error\")\n",
    "        doy=None\n",
    "        raise Exception\n",
    "    return doy.days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_item(lst): # from chatGPT\n",
    "    if not lst:\n",
    "        return None  # handle empty list\n",
    "    \n",
    "    counts = {}\n",
    "    max_count = 0\n",
    "    most_frequent = None\n",
    "\n",
    "    for item in lst:\n",
    "        counts[item] = counts.get(item, 0) + 1\n",
    "        if counts[item] > max_count:\n",
    "            max_count = counts[item]\n",
    "            most_frequent = item\n",
    "\n",
    "    return most_frequent\n",
    "\n",
    "# # Example\n",
    "# data = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\n",
    "# print(most_frequent_item(data))  # Output: \"apple\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsg(file_path): # function to return EPSG code from European Petroleum Survey Group registry\n",
    "        dataset = gdal.Open(file_path)\n",
    "        wkt_crs = dataset.GetProjection()\n",
    "        #print(f\"{wkt_crs}\")\n",
    "\n",
    "        # Optional: Parse it into a spatial reference object\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromWkt(wkt_crs)\n",
    "\n",
    "        #wkt = dataset.GetProjection()\n",
    "\n",
    "        # Try getting the EPSG code\n",
    "        epsg = srs.GetAuthorityCode(None)\n",
    "        #print(f\"epsg: {epsg} wkt_crs {wkt_crs} {file_path}\")\n",
    "        \n",
    "        return epsg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lola(dateStr0,dateStr1,AOIlola,lonCenter,latCenter,work_dir): # function to plot coordinates in longitude, latitude\n",
    "    # Plot the rectangle\n",
    "    #plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([en[0] for en in AOIlola], [EN[1] for EN in AOIlola], marker='o', linestyle='-')\n",
    "    plt.plot(lonCenter,latCenter,marker='*',color='magenta',markersize=24)\n",
    "    #plt.fill([EN[0] for EN in AOIlola], [EN[1] for EN in AOIlola], alpha=0.2, color='blue')\n",
    "    fig.suptitle(f\"{work_dir}\")\n",
    "    plt.title(f\"{dateStr0} to {dateStr1}\")\n",
    "    plt.xlabel(\"Longitude\",fontsize=9)\n",
    "    plt.ylabel(\"Latitude\",fontsize=9)\n",
    "    plt.grid(True)\n",
    "    # Format tick labels to show 3 decimal places\n",
    "    ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
    "    plt.xticks(fontsize=9)\n",
    "    plt.yticks(fontsize=9)\n",
    "\n",
    "    mean_lat = np.radians(np.mean([lola[1] for lola in AOIlola]))\n",
    "    aspect_ratio = 1 / np.cos(mean_lat)\n",
    "\n",
    "    ax.set_aspect(aspect_ratio)\n",
    "    fig.savefig('search1lola.png',dpi=600)\n",
    "    # Display the plot\n",
    "    #fig.show()\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83da711",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    # 2025/06/11 add two more files\n",
    "    files_for_mintpy = ['_water_mask.tif',\n",
    "                        '_corr.tif',\n",
    "                        '_conncomp.tif',\n",
    "                        '_unw_phase.tif',\n",
    "                        '_dem.tif',\n",
    "                        '_lv_theta.tif',\n",
    "                        '_lv_phi.tif'\n",
    "                        '_los_rdr', \n",
    "                        '_wrapped_phase.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zip_files(directory): # function to list zip files after downloading\n",
    "    # Compile the regex pattern to match .zip files\n",
    "    pattern = re.compile(r'.*\\.zip$')\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    # Filter and return files that match the pattern\n",
    "    zip_files = [f for f in all_files if pattern.match(f)]\n",
    "    return zip_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e1c03d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def warp_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float],epsgAOI) -> None: # function to change UTM coordinates \n",
    "    \"\"\"Warp all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-right y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    NODATA_VALUE=0\n",
    "    # Define bounding box in target EPSG coordinates: (minX, minY, maxX, maxY)\n",
    "    #output_bounds = (500000, 4700000, 510000, 4710000)\n",
    "    \n",
    "    # Transform bounds to target CRS (e.g., EPSG:32616)\n",
    "    # transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32616\", always_xy=True)\n",
    "    # xmin_t, ymin_t = transformer.transform(xmin, ymin)\n",
    "    # xmax_t, ymax_t = transformer.transform(xmax, ymax)\n",
    "    output_bounds = (overlap[0], overlap[3], overlap[2], overlap[1])\n",
    "\n",
    "    # 2025/06/06 add los_rdr in hopes that it contains bperp info\n",
    "    # 2025/06/10 add _wrapped_phase.tif \n",
    "    files_for_mintpy = ['_water_mask.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif', '_los_rdr', '_wrapped_phase.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "            # file.name → full file name with extension (e.g., \"example.txt\")\n",
    "            # file.stem → file name without extension (e.g., \"example\")\n",
    "            # file.suffix → extension only (e.g., \".txt\")\n",
    "            epsgTIF1=get_epsg(file)\n",
    "            print(f\"Warping EPSG from {epsgTIF1} to {epsgAOI} on {file.name} to {dst_file.name}\")\n",
    "            gdal.Warp(srcDSOrSrcDSTab=str(file), \n",
    "                      destNameOrDestDS=str(dst_file), \n",
    "                      dstSRS=f\"EPSG:{epsgAOI}\", \n",
    "                      outputBounds=output_bounds, \n",
    "                      srcNodata = NODATA_VALUE,\n",
    "                      dstNodata = NODATA_VALUE,\n",
    "                      )\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57988c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_utm(dateStr0,dateStr1,AOIlola,UTMzone,work_dir,lonCenter,latCenter):   # function to map area of interest - not need\n",
    "\n",
    "    # Create a projection object for UTM zone (you need to specify the zone)\n",
    "    #UTMprojectionFunction = Proj(proj=\"utm\", zone=11, ellps=\"WGS84\", south=False)\n",
    "    UTMprojectionFunction = Proj(proj=\"utm\", zone=UTMzone, ellps=\"WGS84\", south=False)\n",
    "    print(f\"{UTMprojectionFunction}\")\n",
    "\n",
    "    # Convert to CRS and get EPSG code\n",
    "    crsAOI = CRS.from_proj4(UTMprojectionFunction.srs)\n",
    "    epsgAOI = crsAOI.to_epsg()\n",
    "    print(f\"for AOI: EPSG code: {epsgAOI} CRS code {crsAOI} \")\n",
    "\n",
    "\n",
    "    # Project the lat/lon coordinates into UTM\n",
    "    AOIutm = [UTMprojectionFunction(lon, lat) for lon, lat in AOIlola]\n",
    "    ENcenter = UTMprojectionFunction(lonCenter, latCenter)\n",
    "\n",
    "    # define the bounding box\n",
    "    # larger SANEM\n",
    "    #epsgAOI=32611 # UTM Zone 11 \n",
    "    #bBoxAOI = [291074.4825013202, 4480500.202558111, 298595.532410652, 4469090.405787393] \n",
    "    # widen eastward to include GPS station GARL\n",
    "    # Eref =     300196.97\n",
    "    # Nref =    4476704.85\n",
    "\n",
    "    #bBoxAOI = [291.E3, 4480.E3, 301.E3, 4470.E3] \n",
    "    ulx = np.floor(min([corner[0] for corner in AOIutm]))\n",
    "    uly = np.ceil(max([corner[1] for corner in AOIutm]))\n",
    "    lrx = np.ceil(max([corner[0] for corner in AOIutm]))\n",
    "    lry = np.floor(min([corner[1] for corner in AOIutm]))\n",
    "    bBoxAOI=[ulx, uly, lrx, lry]\n",
    "\n",
    "    print(f\"bBoxAOI is {bBoxAOI}\")\n",
    "\n",
    "    # Plot the UTM rectangle\n",
    "    #plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([EN[0]/1000. for EN in AOIutm], [EN[1]/1000. for EN in AOIutm], marker='o', linestyle='-',label='AOI')\n",
    "    plt.fill([EN[0]/1000. for EN in AOIutm], [EN[1]/1000. for EN in AOIutm], alpha=0.2, color='blue')\n",
    "    print(f\"Ecenter {ENcenter[0]/1000.} [km]\")\n",
    "    print(f\"Ncenter {ENcenter[1]/1000.} [km]\")\n",
    "    plt.plot(ENcenter[0]/1000., ENcenter[1]/1000, marker='*', color='magenta',linestyle=None)\n",
    "    plt.suptitle(f\"{work_dir} \\n {dateStr0} to {dateStr1}\", fontsize=9)\n",
    "    plt.title(f\"EPSG code: {epsgAOI} \\n CRS code {crsAOI}\", fontsize=9)\n",
    "    plt.xlabel(\"UTM Easting [km]\")\n",
    "    plt.ylabel(\"UTM Northing [km]\")\n",
    "    # Format tick labels to show 0 decimal places\n",
    "    ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%.0f'))\n",
    "    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.0f'))\n",
    "    plt.grid(False)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    # Place legend outside to the upper right\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "\n",
    "    # save the plot, then show it\n",
    "    plt.savefig('search1utm.png',dpi=600)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return [AOIutm,bBoxAOI,epsgAOI,UTMprojectionFunction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   # command line arguments here\n",
    "    parser = argparse.ArgumentParser(description=\"Script with debug/--no-debug switch.\")\n",
    "    parser.add_argument(\"--site\", dest=\"site\",     default=\"BRADY\", help=\"name of site as 5-character upper-case word\")\n",
    "    parser.add_argument(\"--name\", dest=\"project_name\", default=\"mintpy60\", help=\"label for solution\")\n",
    "    parser.add_argument(\"--aord\", dest=\"aord\",     default=\"ASCENDING\", help=\"flight direction ASCENDING or DESCENDING\")\n",
    "    parser.add_argument(\"--t0\",   dest=\"dateStr0\", default=\"2020-01-01\", help=\"start date in format YYYY-MM-DD\")\n",
    "    parser.add_argument(\"--t1\",   dest=\"dateStr1\", default=\"2020-12-31\", help=\"end   date in format YYYY-MM-DD\")\n",
    " \n",
    "    debug_group = parser.add_mutually_exclusive_group()\n",
    "    debug_group.add_argument('--debug', dest='debug', action='store_true', help=\"Enable debug mode\")\n",
    "    debug_group.add_argument('--no-debug', dest='debug', action='store_false', help=\"Disable debug mode\")\n",
    "    parser.set_defaults(debug=False)  # Default is --no-debug\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.debug:\n",
    "        print(f\"Debug is on\")\n",
    "    else:\n",
    "        print(\"Debug mode is OFF\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ac396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main logic here\n",
    "#\n",
    "# ## set main controlling parameters here\n",
    "# TODO works\n",
    "project_name = 'mintpy63'\n",
    "site = 'BRADY'\n",
    "aord = asf.FLIGHT_DIRECTION.DESCENDING\n",
    "\n",
    "# # TODO fails in mintpy, possibly because very low coherence?\n",
    "# project_name = 'mintpy64'\n",
    "# site = 'LGTDK'\n",
    "# aord = asf.FLIGHT_DIRECTION.DESCENDING\n",
    "# subSwath='IW1'\n",
    "\n",
    "\n",
    "burstORslc = 'BURST'\n",
    "dateStr0='2024-03-01'\n",
    "dateStr1='2024-09-31'\n",
    "debug=True\n",
    "submit=True\n",
    "\n",
    "minTemporalBaseline = 5      # days\n",
    "maxTemporalBaseline = 50    # days # must be greater than excluded season\n",
    "maxPerpendicularBaseline = 50 # meters\n",
    "\n",
    "## consider season - causes problems\n",
    "# take whole year\n",
    "doy1=1\n",
    "doy2=366\n",
    "# exclude January and February\n",
    "#doy1 = day_of_year(parse_date('2023-03-01'))\n",
    "#doy2 = day_of_year(parse_date('2023-12-31'))\n",
    "# take Summer only\n",
    "# doy1 = day_of_year(parse_date('2023-06-01'))\n",
    "# doy2 = day_of_year(parse_date('2023-08-31'))\n",
    "season = [doy1,doy2]\n",
    "# print(f\"season is {season}\")\n",
    "\n",
    "# set up directories\n",
    "if os.path.isdir('/Volumes/feigl/insar'):\n",
    "    topPath='/Volumes/feigl/insar'\n",
    "elif os.path.isdir('/data/insar'):\n",
    "    topPath='/data/insar'\n",
    "\n",
    "# working directory\n",
    "work_dir = Path(topPath) / site / 'SDK' / aord / project_name\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Changing working directory to {work_dir}\")  \n",
    "os.chdir(work_dir)\n",
    "\n",
    "# data directory\n",
    "data_dir = work_dir / 'data'\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"data_dir is {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your [NASA Earthdata login](https://urs.earthdata.nasa.gov/) \n",
    "# to connect to [ASF HyP3](https://hyp3-docs.asf.alaska.edu/).\n",
    "#hyp3 = sdk.HyP3(prompt=True)\n",
    "hyp3 = sdk.HyP3(prompt=False)\n",
    "#hyp3 =  sdk.HyP3.get_authenticated_session('feigl@wisc.edu')\n",
    "nCredits0=hyp3.check_credits()\n",
    "print(f'Number of credits before starting jobs is {nCredits0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b85083",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "get coordinates"
   },
   "outputs": [],
   "source": [
    "LIMITS=get_site_dims(site)\n",
    "print(f\"{LIMITS}\")\n",
    "print(f\"{LIMITS['lonMin']:12.8f}\")\n",
    "print(f\"{LIMITS['UTMzone']}\")\n",
    "\n",
    "if site == 'DCAMP':\n",
    "    # The DAC is expanded to encompass GVR (78 km2) \n",
    "    # -118.3591409949927,38.81917034951293,0 -118.2245378112751,38.82065893511532,0 -118.2251891413302,38.88085324203438,0 -118.3605156707135,38.87935480699949,0 -118.3591409949927,38.81917034951293,0 \n",
    "    # corner\tlon\tlat\tground\n",
    "    # SW\t-118.35914099\t38.81917035\t0\n",
    "    # B\t    -118.22453781\t38.82065894\t0\n",
    "    # NE\t-118.22518914\t38.88085324\t0\n",
    "    # D\t    -118.36051567\t38.87935481\t0\n",
    "    # mean\t-118.29234590\t38.85000933\t0\n",
    "    latMin=38.81917035\n",
    "    latMax=38.88085324\n",
    "    lonMin=-118.36051567\n",
    "    lonMax=-118.22453781\n",
    "    UTMzone=11 # 11 S verified by Google Earth\n",
    "elif site == 'SANEM':\n",
    "    # original AOI\n",
    "    #  grep -A3 sanem ~/siteinfo/site_dims.txt  \n",
    "    # sanem:\n",
    "    # -R-119.46/-119.375/40.348/40.449\n",
    "    # -R291074.48226/298595.53221/4469090.38971/4480500.18609\n",
    "    # 11\n",
    "    # latMin=40.348\n",
    "    # latMax=40.449\n",
    "    # lonMin=-119.46\n",
    "    # lonMax=-119.375 \n",
    "    \n",
    "    # larger AOI from /Users/feigl/siteinfo/sanem/AOIforSANEM2025.kml  \n",
    "    # -119.5086551479411,40.3051742449493,0\n",
    "    # -119.3250213982107,40.30569311996287,0\n",
    "    # -119.3270788642407,40.50172708513533,0\n",
    "    # -119.5083119649335,40.5004502253031,0\n",
    "    # -119.5086551479411,40.3051742449493,0\n",
    "    latMin=40.30517424494930\n",
    "    latMax=40.50172708513533\n",
    "    lonMin=-119.5086551479411\n",
    "    lonMax=-119.3250213982107\n",
    "    UTMzone=11      # 2025/06/05  verified by Google Earth\n",
    "elif site == 'MCGIN':\n",
    "    UTMzone=11 # 11 S verified by Google Earth\n",
    "    assert False\n",
    "elif site == 'BRADY':\n",
    "    UTMzone=LIMITS['UTMzone']\n",
    "    LIMITS=get_site_dims(site)\n",
    "    # \n",
    "    lonMin=LIMITS['lonMin'] \n",
    "    lonMax=LIMITS['lonMax'] \n",
    "    latMin=LIMITS['latMin'] \n",
    "    latMax=LIMITS['latMax']\n",
    "    # # expand by 10 km - fails\n",
    "    # lonMin=LIMITS['lonMin'] - 10/111.\n",
    "    # lonMax=LIMITS['lonMax'] + 10/111.\n",
    "    # latMin=LIMITS['latMin'] - 10/111.\n",
    "    # latMax=LIMITS['latMax'] + 10/111.\n",
    "    #print(f\"{LIMITS['lonMin']:12.8f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"WARNING unknown site {site}\")\n",
    "    UTMzone=LIMITS['UTMzone']\n",
    "    LIMITS=get_site_dims(site)\n",
    "    lonMin=LIMITS['lonMin']\n",
    "    lonMax=LIMITS['lonMax']\n",
    "    latMin=LIMITS['latMin']\n",
    "    latMax=LIMITS['latMax']\n",
    "\n",
    "print(f\"UTMzone is {UTMzone}\")   \n",
    "lonCenter=(lonMin + lonMax)/2\n",
    "latCenter=(latMin + latMax)/2\n",
    "# format is intersectsWith='POINT(-119.543 37.925)'\n",
    "centerAOIWKT=f'POINT({lonCenter} {latCenter})'\n",
    "print(f\"centerAOIWKT is {centerAOIWKT}\")\n",
    "\n",
    "# Define the four corners\n",
    "AOIlola = [\n",
    "    [lonMin, latMin],  # Bottom-left corner\n",
    "    [lonMax, latMin],  # Bottom-right corner\n",
    "    [lonMax, latMax],  # Top-right corner\n",
    "    [lonMin, latMax],  # Top-left corner\n",
    "    [lonMin, latMin]   # Close the loop by returning to bottom-left\n",
    "]\n",
    "\n",
    "# make plots\n",
    "plot_lola(dateStr0,dateStr1,AOIlola,lonCenter,latCenter,work_dir)\n",
    "AOIutm,bBoxAOI,epsgAOI,UTMprojectionFunction = plot_utm(dateStr0,dateStr1,AOIlola,UTMzone,work_dir,lonCenter,latCenter)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    print(\"Debug mode is ON. Continuing bravely onward.\")\n",
    "else:\n",
    "    print(\"Debug mode is OFF. Continuing bravely onward.\")  \n",
    "    #sys.exit(f\"Debug mode is off. Exiting here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"timeout is now {asf.constants.INTERNAL.CMR_TIMEOUT} seconds\")\n",
    "asf.constants.INTERNAL.CMR_TIMEOUT=120\n",
    "print(f\"timeout is now {asf.constants.INTERNAL.CMR_TIMEOUT}: seconds\")\n",
    "\n",
    "if burstORslc == 'BURST':\n",
    "    processingLevel=asf.PRODUCT_TYPE.BURST\n",
    "# elif burstORslc == 'MULTIBURST':\n",
    "#     processingLevel=asf.PRODUCT_TYPE.BURST\n",
    "elif burstORslc == 'SLC':\n",
    "    processingLevel=asf.PRODUCT_TYPE.SLC\n",
    "else:\n",
    "    assert False # throw an error\n",
    "ProductsFound = asf.geo_search(\n",
    "        platform=asf.PLATFORM.SENTINEL1,\n",
    "        intersectsWith=centerAOIWKT,\n",
    "        start=dateStr0,\n",
    "        end  =dateStr1,        \n",
    "        processingLevel=processingLevel,\n",
    "        polarization=asf.POLARIZATION.VV, \n",
    "        beamMode=asf.BEAMMODE.IW,\n",
    "        flightDirection=aord,\n",
    "    )\n",
    "        # season=season,\n",
    "\n",
    "nProducts=len(ProductsFound)\n",
    "print(f'nProducts = {nProducts}')\n",
    "if nProducts <= 0:\n",
    "    print(f\"ERROR: no products found\")\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f84287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for product in ProductsFound: # filter by subswath\n",
    "#     #print(f\"File: {product.properties['fileName']}\")\n",
    "#     #print(f\"Product ID: {product.properties['productID']}\")\n",
    "#     #print(f\"IW Swath: {product.properties.get('subswath')}\") # Note: This field may not be present in all products\n",
    "\n",
    "#     match = re.search(r\"IW\\d+\", product.properties['fileName'])\n",
    "#     if match:\n",
    "#         iw = match.group(0)   # e.g., 'IW3'\n",
    "#         iw_num = int(iw[2:])  # e.g., 3\n",
    "#         # print(\"Full match:\", iw)\n",
    "#         # print(\"Number only:\", iw_num)\n",
    "#         product.properties['swath']=iw\n",
    "#     else:\n",
    "#         print(\"No IW pattern found\")\n",
    "        \n",
    "# ProductsFound = [product for product in ProductsFound if product.properties['swath'] != subSwath]\n",
    "\n",
    "# nProducts=len(ProductsFound)\n",
    "# print(f'nProducts = {nProducts}')\n",
    "# if nProducts > 0:\n",
    "#     for product in ProductsFound:\n",
    "#         print(f\"{product.properties['fileName']}\")\n",
    "# else:\n",
    "#     print(f\"ERROR: no products found\")\n",
    "#     raise Exception\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce83439",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map coordinates in longitude and latitude\n",
    "#print(f'{AOIlola}')\n",
    "#print(f'{coordinates}')\n",
    "plt.figure()\n",
    "plt.plot([point[0] for point in AOIlola], [point[1] for point in AOIlola], marker='o', linestyle='-')\n",
    "plt.fill([point[0] for point in AOIlola], [point[1] for point in AOIlola], alpha=0.2, color='blue')\n",
    "plt.plot(lonCenter,latCenter,marker='*',color='magenta',markersize=12)\n",
    "\n",
    "# plot the coordinates for each scene\n",
    "for product in ProductsFound: \n",
    "    coordsLOLA=product.geometry['coordinates']   \n",
    "    plt.plot([point[0] for point in coordsLOLA[0]], [point[1] for point in coordsLOLA[0]], marker='+', linestyle='-')\n",
    "\n",
    "plt.suptitle(f\"{work_dir}\")\n",
    "plt.title(f\"{dateStr0} to {dateStr1}\")\n",
    "plt.xlabel(\"UTM Easting [km]\")\n",
    "plt.ylabel(\"UTM Northing [km]\")\n",
    "# Format tick labels to show 3 decimal places\n",
    "#plt.gca().xaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "#plt.gca().yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))\n",
    "plt.grid(False)\n",
    "# save the plot, then show it\n",
    "plt.savefig('map_lola.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbfd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map coverage in UTM and filter based on intersection \n",
    "# Project the lat/lon coordinates into UTM\n",
    "AOIutm = [UTMprojectionFunction(lon, lat) for lon, lat in AOIlola]\n",
    "Ecenter, Ncenter =  UTMprojectionFunction(lonCenter, latCenter)\n",
    "\n",
    "# make polygon\n",
    "AOIpolygon = shapely.Polygon(AOIutm)\n",
    "AOIarea = AOIpolygon.area\n",
    "print(f\"AOI has area of {AOIarea/1.E6:.0f} [km^2]\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot([point[0]/1000 for point in AOIutm], [point[1]/1000 for point in AOIutm], marker='o', linestyle='-')\n",
    "plt.fill([point[0]/1000 for point in AOIutm], [point[1]/1000 for point in AOIutm], alpha=0.2, color='blue',label='AOI')\n",
    "plt.plot(Ecenter/1000,Ncenter/1000,marker='*',color='magenta',markersize=12,label='AOI center')\n",
    "\n",
    "nKeep=0\n",
    "nSkip=0\n",
    "\n",
    "ProductsKept = empty_object = type(ProductsFound)()\n",
    "for product in ProductsFound:           \n",
    "    #print(f\"{product.properties['sceneName']}\")  \n",
    "    coordsLOLA=product.geometry['coordinates']\n",
    "    coordsUTM=[UTMprojectionFunction(lola[0],lola[1]) for lola in coordsLOLA[0]]\n",
    "    polygon1 = shapely.Polygon(coordsUTM)\n",
    "    \n",
    "    # find intersection and its area\n",
    "    overlap=shapely.intersection(polygon1,AOIpolygon)\n",
    "    area1=overlap.area\n",
    "    print(f\"{product.properties['sceneName']} overlaps AOI with area of {area1/1.E6:.0f} [km^2]\")\n",
    "    \n",
    "    # keep if burst covers more than half of AOI\n",
    "    if area1/AOIarea >= 0.5:\n",
    "        nKeep=nKeep+1\n",
    "        ProductsKept.append(product)\n",
    "    else:\n",
    "        nSkip=nSkip+1\n",
    "        \n",
    "    # plot the coordinates for this scene\n",
    "    plt.plot([point[0]/1000 for point in coordsUTM], [point[1]/1000 for point in coordsUTM], marker='+', linestyle='-')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "# #plt.title(f\"{granule.properties['sceneName']}\")\n",
    "plt.suptitle(f\"{work_dir}\")\n",
    "plt.title(f\"{dateStr0} to {dateStr1} nKeep = {nKeep} nSkip = {nSkip}\")\n",
    "plt.xlabel(\"UTM Easting [km]\")\n",
    "plt.ylabel(\"UTM Northing [km]\")\n",
    "plt.grid(True)\n",
    "plt.savefig('search_utm2.png')\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of products found is {len(ProductsFound)}\")\n",
    "print(f\"Number of products kept  is {len(ProductsKept)}\")\n",
    "print(f\"nKeep  is                   {nKeep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'asf.constants.INTERNAL.CMR_TIMEOUT is {asf.constants.INTERNAL.CMR_TIMEOUT} seconds')\n",
    "asf.constants.INTERNAL.CMR_TIMEOUT=120\n",
    "print(f'asf.constants.INTERNAL.CMR_TIMEOUT is {asf.constants.INTERNAL.CMR_TIMEOUT} seconds')\n",
    "\n",
    "# make a stack of epochs\n",
    "print(f\"number of products is {len(ProductsKept)}\")\n",
    "# This will make a stack of ALL possible pairs that use last epoch as reference\n",
    "EpochsAll = asf.baseline_search.stack_from_product(ProductsKept[-1])\n",
    "# print(f\"{StackAll.Properties.values}\")\n",
    "                                                            \n",
    "nStackAll = len(EpochsAll)\n",
    "print(f\"nStackAll is {nStackAll}\")\n",
    "\n",
    "\n",
    "# trim list of epochs\n",
    "t0 = parse_date(dateStr0 + ' 00:00:00Z')\n",
    "t1 = parse_date(dateStr1 + ' 23:59:59Z')\n",
    "\n",
    "EpochsSub=empty_object=type(EpochsAll)()\n",
    "for Epoch0 in EpochsAll:\n",
    "    #print(f\"baseline is {baseline}\")\n",
    "    if ((parse_date(Epoch0.properties['startTime']) >= t0) \n",
    "        and (parse_date(Epoch0.properties['stopTime']) <= t1) \n",
    "        and Epoch0.properties['perpendicularBaseline'] != None):\n",
    "        EpochsSub.append(Epoch0)\n",
    "    \n",
    "nStackSub=len(EpochsSub)\n",
    "print(f\"nStackSub is {nStackSub}\")\n",
    "\n",
    "# for Epoch0 in EpochsSub:\n",
    "#     #print(f\"{Epoch0.properties}\") \n",
    "#     print(f\"{Epoch0.properties['startTime']} {Epoch0.properties['temporalBaseline']:5d}days {Epoch0.properties['perpendicularBaseline']:10.1f}m {Epoch0.properties['burst']['fullBurstID']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e795ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start building set of pairs\n",
    "Pairs = set()\n",
    "# make a set adding to the end\n",
    "for Epoch0 in EpochsSub:\n",
    "#print(f\"{Epoch0.properties}\") \n",
    "\n",
    "    rN=Epoch0.properties['sceneName']             # long name of burst granule\n",
    "    rt=Epoch0.properties['temporalBaseline']      # days from first epoch for reference\n",
    "    rB=Epoch0.properties['perpendicularBaseline'] # meters from first epoch for reference\n",
    "    rd=day_of_year(parse_date(Epoch0.properties['startTime'])) # day of year \n",
    "    rS=Epoch0.properties['burst']['subswath']\n",
    "    ri=Epoch0.properties['burst']['burstIndex']           # 5, 6, 7\n",
    "    rF=Epoch0.properties['burst']['fullBurstID']\n",
    "\n",
    "    for Epoch1 in EpochsSub:\n",
    "        sN=Epoch1.properties['sceneName']             # long name of burst granule\n",
    "        st=Epoch1.properties['temporalBaseline']      # days from first epoch for reference\n",
    "        sB=Epoch1.properties['perpendicularBaseline'] # meters from first epoch for reference\n",
    "        sd=day_of_year(parse_date(Epoch1.properties['startTime'])) # day of year \n",
    "        sS=Epoch1.properties['burst']['subswath']   # 'IW1' 'IW2' or 'IW3'\n",
    "        si=Epoch1.properties['burst']['burstIndex']           # 5, 6, 7\n",
    "        sF=Epoch1.properties['burst']['fullBurstID']\n",
    "        \n",
    "\n",
    "        if ((sN != rN) and (si == ri) and (sS == rS) and (sF == rF)\n",
    "            and (abs(sB - rB) < maxPerpendicularBaseline)\n",
    "            and (st - rt <= maxTemporalBaseline)\n",
    "            and (st - rt >  minTemporalBaseline)\n",
    "            and (rd >= season[0])\n",
    "            and (rd <= season[1])\n",
    "            ):\n",
    "            print(f\"{rN}, {sN}, {abs(sB - rB):10.1f}m, {(st-rt):5d}days {ri}, {rS}, {rF}\")\n",
    "            Pairs.add((rN,sN) )\n",
    "            \n",
    "nPairs=len(Pairs)\n",
    "print(f'number of pairs nPairs = {nPairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad87547",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs=hyp3.costs()\n",
    "print(f\"costs is of type {type(costs)}\")\n",
    "print(f\"{costs}\")\n",
    "for cost in costs:\n",
    "    print(f\"{cost}\")\n",
    "    #     for p in cost:\n",
    "    #         print(p,end=\"\\n\")\n",
    "    #     print()\n",
    "# The number of looks drives the resolution and pixel spacing of the output products. \n",
    "# Selecting 10x2 looks will yield larger products with 80 m resolution and pixel spacing of 40 m. \n",
    "# Selecting 20x4 looks reduces the resolution to 160 m and reduces the size of the products (roughly 1/4 the size of 10x2 look products), with a pixel spacing of 80 m. \n",
    "# The default is 20x4 looks.\n",
    "# \n",
    "# Modifying looks does not change the cost!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fdf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18143bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Prepare and submit each batch job on SDK\n",
    "# #help(sdk.Batch)\n",
    "jobName=project_name\n",
    "print(f'Preparing insar burst jobs with name {project_name}')\n",
    "nCredits0 = hyp3.check_credits()\n",
    "print(f'nCredits0 = {nCredits0}')\n",
    "#looks='20x4'\n",
    "looks='10x2'\n",
    "jobs = sdk.Batch()\n",
    "nJobs=0\n",
    "nJobsSubmitted=0\n",
    "for Epoch0, Epoch1 in Pairs:\n",
    "    nJobs=nJobs+1\n",
    "    #jobName=\"%s_job%02d\" % (project_name, nJobs)\n",
    "    # new in hyp3_sdk v7.4.0 API Reference\n",
    "    #https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.HyP3.prepare_insar_isce_multi_burst_job\n",
    "    print(f\"{nJobs:5d} : {Epoch0} to {Epoch1}\")\n",
    "    if burstORslc == 'BURST':\n",
    "        job=hyp3.prepare_insar_isce_burst_job(Epoch0, Epoch1, \n",
    "            name=jobName, \n",
    "            apply_water_mask=True,\n",
    "            looks=looks)\n",
    "        # jobs+=hyp3.submit_insar_isce_burst_job(Epoch0, Epoch1, \n",
    "        #     name=jobName, \n",
    "        #     apply_water_mask=True,\n",
    "        #     looks=looks)\n",
    "        costFor1Job=1\n",
    "    elif burstORslc == 'MULTIBURST':     \n",
    "            #     prepare_insar_isce_multi_burst_job(reference, secondary, name=None, apply_water_mask=False, looks='20x4') classmethod ¶\n",
    "            # Prepare an InSAR ISCE multi burst job.\n",
    "\n",
    "            # Parameters:\n",
    "\n",
    "            # Name\tType\tDescription\tDefault\n",
    "            # reference\tlist[str]\tA list of reference granules (scenes) to use\trequired\n",
    "            # secondary\tlist[str]\tA list of secondary granules (scenes) to use\trequired\n",
    "            # name\tstr | None\tA name for the job\tNone\n",
    "            # apply_water_mask\tbool\tSets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping\tFalse\n",
    "            # looks\tLiteral['20x4', '10x2', '5x1']\tNumber of looks to take in range and azimuth\n",
    "        job=prepare_insar_isce_multi_burst_job(Epoch0, Epoch1, \n",
    "            name=jobName, \n",
    "            looks=looks)\n",
    "        costFor1Job=1\n",
    "    elif burstORslc == 'SLC':\n",
    "        job=hyp3.prepare_insar_job(Epoch0, Epoch1, \n",
    "                                name=jobName, \n",
    "                                looks=looks, \n",
    "                                include_look_vectors=True, \n",
    "                                include_inc_map=True, \n",
    "                                include_dem=True, \n",
    "                                include_wrapped_phase=True, \n",
    "                                apply_water_mask=False,\n",
    "                                include_displacement_maps=True, \n",
    "                                phase_filter_parameter=0.6) \n",
    "        costFor1Job=15\n",
    "    else:\n",
    "        assert False # throw error\n",
    "    \n",
    "    if submit == True:\n",
    "        jobs+=hyp3.submit_prepared_jobs(job)\n",
    "        nJobsSubmitted=nJobsSubmitted+1\n",
    "    else:\n",
    "        print(f\"submit is False. Not submitting this job.\")\n",
    "\n",
    "if submit == True:\n",
    "    print(f'submitted nJobs is {nJobs}')\n",
    "    print(f'{nJobsSubmitted}')\n",
    "    print(f'{len(jobs)}')\n",
    "    costEstimate=nJobs*costFor1Job # TODO use cost value from table, type of job and possibly number of looks\n",
    "    print(f'costEstimate is {costEstimate}')\n",
    "else:\n",
    "    print(f\"submit is False. Stopping here.\")\n",
    "    raise Exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.watch(jobs)\n",
    "nCredits1 = hyp3.check_credits()\n",
    "print(f'nCredits1 = {nCredits1}')\n",
    "nCreditsUsed=nCredits1-nCredits0\n",
    "print(f'nCreditsUsed = {nCreditsUsed}')\n",
    "#print(f'costEstimate is {costEstimate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5907c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.find_jobs(name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfded378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download \n",
    "insar_products = jobs.download_files(data_dir)\n",
    "print(f'data_dir is {data_dir}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c52a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd530472",
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_products = data_dir.glob('*.zip')\n",
    "print(f'{insar_products}')\n",
    "\n",
    "insar_products = [sdk.util.extract_zipped_product(ii,delete=False) for ii in insar_products]\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40207d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ASFHyP3/hyp3-docs/blob/main/docs/tutorials/hyp3_isce2_burst_stack_for_ts_analysis.ipynb\n",
    "def get_common_overlap_intersect(fileList: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "\n",
    "    Arg:\n",
    "        fileList: a list of GeoTIFF files\n",
    "\n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    gdal.UseExceptions()\n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in fileList]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea78d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_overlap_union(fileList: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "\n",
    "    Arg:\n",
    "        fileList: a list of GeoTIFF files\n",
    "\n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "         \n",
    "         from https://github.com/ASFHyP3/hyp3-docs/blob/main/docs/tutorials/hyp3_isce2_burst_stack_for_ts_analysis.ipynb\n",
    "         \n",
    "         # updated 2025/05/17\n",
    "    \"\"\"\n",
    "\n",
    "    gdal.UseExceptions()\n",
    "\n",
    "    #print(f\"fileList is {fileList}\")\n",
    "    \n",
    "    # for dem in fileList[0]:\n",
    "    #     info = gdal.Info(str(dem), format='json') \n",
    "    #     #print(f\"dem is {dem} info is {info}\")\n",
    "    #     #'coordinateSystem': {'wkt': 'PROJCRS[\"WGS 84 / UTM zone 10N\",\\n\n",
    "    #     print(f\" {info['coordinateSystem']['wkt']}\")\n",
    "    #     #print(f\" {info['coordinateSystem']['wkt']['PROJCRS']}\")\n",
    "\n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in fileList]\n",
    "    \n",
    "    ulx = min(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = max(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = max(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = min(corner['lowerRight'][1] for corner in corners)\n",
    "    \n",
    "    \n",
    "    print(f\"{[ulx, uly, lrx, lry]}\")\n",
    "    \n",
    "         \n",
    "    return [ulx, uly, lrx, lry]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a05b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.UseExceptions()\n",
    "print(f'data_dir is {data_dir}')\n",
    "fileList = data_dir.glob('*/*_dem.tif')\n",
    "\n",
    "epsgTIFs = [get_epsg(file_path) for file_path in fileList]\n",
    "print(f\"{epsgTIFs}\")\n",
    "\n",
    "epsgTIF1=np.unique(epsgTIFs)\n",
    "print(f\"{epsgTIF1}\")\n",
    "\n",
    "print(f\"in EPSG {epsgAOI}  bBoxAOI is     {bBoxAOI}\")\n",
    "fileList = data_dir.glob('*/*_dem.tif')\n",
    "bBoxUnion = get_common_overlap_union(fileList)\n",
    "print(f\"in EPSG {epsgTIF1} bBoxUnion is     {bBoxUnion}\")\n",
    "fileList = data_dir.glob('*/*_dem.tif')\n",
    "bBoxIntersect = get_common_overlap_intersect(fileList)\n",
    "print(f\"in EPSG {epsgTIF1} bBoxIntersect is {bBoxIntersect}\")\n",
    "\n",
    "if len(epsgTIF1) == 1:\n",
    "    epsgTIF1=int(epsgTIF1[0]) # convert list to integer\n",
    "    print(f\"epsgTIF1 is {epsgTIF1}\")  \n",
    "    # print(f\"EPSG codes in TIF files {epsgTIF1} matches EPSG code in AOI {epsgAOI}. Starting to clip TIF files to Union...\")\n",
    "    # clip_hyp3_products_to_common_overlap(data_dir, bBoxUnion)\n",
    "    # take everything\n",
    "    # print(f\"EPSG codes in TIF files {epsgTIF1} matches EPSG code in AOI {epsgAOI}. Starting to clip TIF files to Intersection...\")\n",
    "    # clip_hyp3_products_to_common_overlap(data_dir, bBoxIntersect)\n",
    "    # leads to error downstream ::\n",
    "    # ValueError: could not broadcast input array from shape (367,287) into shape (1004,2647)\n",
    "    print(f\"EPSG codes in TIF files {epsgTIF1} matches EPSG code in AOI {epsgAOI}. Starting to clip TIF files...\")\n",
    "    clip_hyp3_products_to_common_overlap(data_dir, bBoxAOI)\n",
    "\n",
    "else:\n",
    "    print(f\"ERROR TIF files do not all have the same EPSG codes. {len(epsgTIF1)}\")\n",
    "    for file1, epsg1 in zip(fileList, epsgTIFs):\n",
    "        print(f\"file {file1} has EPSG code {epsg1}\")\n",
    "        \n",
    "    \n",
    "    print(f\"EPSG codes in TIF files {epsgTIF1} does NOT match EPSG code in AOI {epsgAOI}. Starting to warp TIF files slowly.... \")\n",
    "    warp_hyp3_products_to_common_overlap(data_dir, bBoxAOI, epsgAOI)\n",
    "    \n",
    "    #sys.exit(f\"Exiting here.\")  \n",
    "    #raise Exception\n",
    "\n",
    "  \n",
    "print(f\"Done with TIF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a909572",
   "metadata": {},
   "outputs": [],
   "source": [
    "mintpy_config = work_dir / 'mintpy_config.txt'\n",
    "mintpy_config.write_text(\n",
    "f\"\"\"\n",
    "mintpy.load.processor        = hyp3\n",
    "##---------interferogram datasets:\n",
    "mintpy.load.unwFile          = {data_dir}/*/*_unw_phase_clipped.tif\n",
    "mintpy.load.corFile          = {data_dir}/*/*_corr_clipped.tif\n",
    "mintpy.load.connCompFile     = {data_dir}/*/*_conncomp_clipped.tif\n",
    "##---------geometry datasets:\n",
    "mintpy.load.demFile          = {data_dir}/*/*_dem_clipped.tif\n",
    "mintpy.load.incAngleFile     = {data_dir}/*/*_lv_theta_clipped.tif\n",
    "mintpy.load.azAngleFile      = {data_dir}/*/*_lv_phi_clipped.tif\n",
    "mintpy.load.waterMaskFile    = {data_dir}/*/*_water_mask_clipped.tif\n",
    "mintpy.troposphericDelay.method = no\n",
    "##---------misc:\n",
    "mintpy.plot = no\n",
    "mintpy.network.coherenceBased = no\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!smallbaselineApp.py --dir {work_dir} {mintpy_config}\n",
    "# !mamba run -n mintpy smallbaselineApp.py --dir {work_dir} {mintpy_config}\n",
    "# %matplotlib widget\n",
    "# from mintpy.cli import view, tsview\n",
    "# view.main([f'{work_dir}/velocity.h5'])\n",
    "# tsview.main([f'{work_dir}/timeseries.h5'])\n",
    "\n",
    "# rm -rf inputs pic *.h5\n",
    "# mamba run -n mintpy smallbaselineApp.py ${runname}.cfg > ${runname}.out 2> ${runname}.err &\n",
    "\n",
    "# ValueError: Invalid NaN value found in the following kept pairs for Bperp or coherence! \n",
    "#         They likely have issues, check them and maybe exclude them in your network.\n",
    "#         ['20220910_20221016']\n",
    "# ls data -d | grep 20220910 | grep 20221016\n",
    "# ls: -d: No such file or directory\n",
    "# S1_135553_IW2_20220910_20221016_VV_INT40_6D3B\n",
    "# S1_135553_IW2_20220910_20221016_VV_INT40_6D3B.zip\n",
    "# (base) brady:mintpy61 feigl$ mkdir data_bad\n",
    "# (base) brady:mintpy61 feigl$ mv data/S1_135553_IW2_20220910_20221016_VV_INT40_6D3B data_bad\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "hyp3kf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
